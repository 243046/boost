{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e02323",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d551e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa994a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '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'\n",
    "usr = '243046'\n",
    "repo = 'boost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://{token}@github.com/{usr}/{repo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a28cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df272e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c62b3a",
   "metadata": {},
   "source": [
    "# No search - all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from wrappers.datasets_models_wrappers import DataModelsWrapper, DataModelsWrapperRandomSearch\n",
    "from wrappers.datasets_models_wrappers_nlp import DataModelsWrapperNLP, DataModelsWrapperNLPRandomSearch\n",
    "from data_processing.process_dataset import prepare_datasets_for_classification\n",
    "from data_processing.process_dataset_nlp import prepare_nlp_for_classification\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def run(param_dict, mode='randomized', tuner='hyperopt', scoring='accuracy'):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperRandomSearch(param_dict, scoring=scoring)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapper(param_dict, tuner=tuner, scoring=scoring)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting\n",
    "\n",
    "\n",
    "def run_nlp(param_dict, mode='randomized', tuner='hyperopt', scoring='accuracy',\n",
    "            tfidf_kws={'ngram_range': (1, 2), 'min_df': 3, 'max_features': 10000}):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperNLPRandomSearch(param_dict, scoring=scoring, tfidf_kws=tfidf_kws)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapperNLP(param_dict, tuner=tuner, scoring=scoring, tfidf_kws=tfidf_kws)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b55e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'mushrooms.csv': ('class', 'all', None),\n",
    "    'adult.csv': ('profit', [], None),\n",
    "    'churn.csv': ('Churn', [], None),\n",
    "    'creditcard.csv': ('Class', [], None),\n",
    "    'prostate.csv': ('target', [], None),\n",
    "    'leukemia.csv': ('target', [], None),\n",
    "    'weather_dataset.csv': ('target', [], 200)\n",
    "}\n",
    "\n",
    "X_1, y_1, X_2, y_2, X_3, y_3, X_4, y_4, X_5, y_5, X_6, y_6, X_7, y_7 = prepare_datasets_for_classification(d, data_path='data/')\n",
    "\n",
    "X_8, y_8 = prepare_nlp_for_classification(\n",
    "    dataset_name='imdb_dataset.csv',\n",
    "    text_column='review_cleared',\n",
    "    y_col='sentiment',\n",
    "    nrows=2000,\n",
    "    data_path='data/'\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(), {}),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                              random_state=123), {}),\n",
    "    'LightGBM': (LGBMClassifier(), {}),\n",
    "    'CatBoost': (CatBoostClassifier(n_estimators=100, verbose=False,\n",
    "                                    random_state=123), {})\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    'mushrooms': (X_1, y_1, models),\n",
    "    'adult': (X_2, y_2, models),\n",
    "    'churn': (X_3, y_3, models),\n",
    "    'credit card': (X_4, y_4, models),\n",
    "    'prostate': (X_5, y_5, models),\n",
    "    'leukemia': (X_6, y_6, models),\n",
    "    'weather': (X_7, y_7, models)\n",
    "}\n",
    "\n",
    "param_dict_nlp = {\n",
    "    'IMDB reviews': (X_8, y_8, models)\n",
    "}\n",
    "\n",
    "tfidf_kws = {'ngram_range': (1, 2), 'min_df': 3, 'max_features': 10000}\n",
    "\n",
    "all_results, all_runtimes, results_for_plotting, runtimes_for_plotting = run(param_dict=param_dict,\n",
    "                                                                             mode='randomized', scoring='accuracy')\n",
    "\n",
    "_, _, results_for_plotting_nlp, runtimes_for_plotting_nlp = run_nlp(param_dict=param_dict_nlp,\n",
    "                                                                    mode='randomized',\n",
    "                                                                    scoring='accuracy',\n",
    "                                                                    tfidf_kws=tfidf_kws\n",
    "                                                                    )\n",
    "\n",
    "all_results = pd.concat([results_for_plotting, results_for_plotting_nlp])\n",
    "all_runtimes = pd.concat([runtimes_for_plotting, runtimes_for_plotting_nlp])\n",
    "all_results.to_excel('results_colab/results_no_search.xlsx', index=False)\n",
    "all_runtimes.to_excel('results_colab/runtimes_no_search.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09628a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download('results_colab/results_no_search.xlsx')\n",
    "files.download('results_colab/runtimes_no_search.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82b7ad",
   "metadata": {},
   "source": [
    "# Ordinary TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from ray import tune\n",
    "\n",
    "from wrappers.datasets_models_wrappers import DataModelsWrapper, DataModelsWrapperRandomSearch\n",
    "from data_processing.process_dataset import prepare_datasets_for_classification\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def run(param_dict, mode='TPE', tuner='hyperopt', scoring='accuracy'):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperRandomSearch(param_dict, scoring=scoring)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapper(param_dict, tuner=tuner, scoring=scoring)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'mushrooms.csv': ('class', 'all', None),\n",
    "    'adult.csv': ('profit', [], None),\n",
    "    'churn.csv': ('Churn', [], None),\n",
    "    'creditcard.csv': ('Class', [], None)\n",
    "}\n",
    "\n",
    "X_1, y_1, X_2, y_2, X_3, y_3, X_4, y_4 = prepare_datasets_for_classification(d, data_path='data/')\n",
    "\n",
    "boosting_params = {\n",
    "    'n_estimators': tune.choice([50, 100, 150]),\n",
    "    'learning_rate': tune.loguniform(0.01, 0.1)\n",
    "}\n",
    "xgb_params = {\n",
    "    'n_estimators': tune.choice([50, 100, 150]),\n",
    "    'learning_rate': tune.loguniform(0.01, 0.1)\n",
    "}\n",
    "lgbm_params = {\n",
    "    'n_estimators': tune.choice([50, 100, 150]),\n",
    "    'learning_rate': tune.loguniform(0.01, 0.1)\n",
    "}\n",
    "catboost_params = {\n",
    "    'n_estimators': tune.choice([50, 100, 150]),\n",
    "    'learning_rate': tune.loguniform(0.01, 0.1)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(), boosting_params),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False,\n",
    "                             eval_metric='logloss', random_state=123), xgb_params),\n",
    "    'LightGBM': (LGBMClassifier(), lgbm_params),\n",
    "    'CatBoost': (CatBoostClassifier(verbose=False, random_state=123), catboost_params)\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    'mushrooms': (X_1, y_1, models),\n",
    "    'adult': (X_2, y_2, models),\n",
    "    'churn': (X_3, y_3, models),\n",
    "    'credit card': (X_4, y_4, models)\n",
    "}\n",
    "\n",
    "all_results, all_runtimes, results_for_plotting, runtimes_for_plotting = run(param_dict=param_dict,\n",
    "                                                                             mode='TPE', scoring='accuracy')\n",
    "\n",
    "name = 'ordinary_TPE'\n",
    "results_for_plotting.to_excel(f'results_colab/results_{name}.xlsx', index=False)\n",
    "runtimes_for_plotting.to_excel(f'results_colab/runtimes_{name}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(f'results_colab/results_{name}.xlsx')\n",
    "files.download(f'results_colab/runtimes_{name}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29361ce",
   "metadata": {},
   "source": [
    "# High dimensional TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e941de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from ray import tune\n",
    "\n",
    "from wrappers.datasets_models_wrappers import DataModelsWrapper, DataModelsWrapperRandomSearch\n",
    "from wrappers.datasets_models_wrappers_nlp import DataModelsWrapperNLP, DataModelsWrapperNLPRandomSearch\n",
    "from data_processing.process_dataset import prepare_datasets_for_classification\n",
    "from data_processing.process_dataset_nlp import prepare_nlp_for_classification\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def run(param_dict, mode='TPE', tuner='hyperopt', scoring='accuracy'):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperRandomSearch(param_dict, scoring=scoring)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapper(param_dict, tuner=tuner, scoring=scoring)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting\n",
    "\n",
    "\n",
    "def run_nlp(param_dict, mode='TPE', tuner='hyperopt', scoring='accuracy',\n",
    "        tfidf_kws={'ngram_range': (1, 2), 'min_df': 3, 'max_features': 10000}):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperNLPRandomSearch(param_dict, scoring=scoring, tfidf_kws=tfidf_kws)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapperNLP(param_dict, tuner=tuner, scoring=scoring, tfidf_kws=tfidf_kws)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'prostate.csv': ('target', [], None),\n",
    "    'leukemia.csv': ('target', [], None),\n",
    "    'weather_dataset.csv': ('target', [], 500)\n",
    "}\n",
    "\n",
    "X_1, y_1, X_2, y_2, X_3, y_3 = prepare_datasets_for_classification(d, data_path='data/')\n",
    "\n",
    "X_4, y_4 = prepare_nlp_for_classification(\n",
    "    dataset_name='imdb_dataset.csv',\n",
    "    text_column='review_cleared',\n",
    "    y_col='sentiment',\n",
    "    nrows=3000,\n",
    "    data_path='data/'\n",
    ")\n",
    "\n",
    "boosting_params = {\n",
    "    'subsample': tune.uniform(0.6, 0.8)\n",
    "}\n",
    "xgb_params = {\n",
    "    'reg_alpha': tune.loguniform(1, 10),\n",
    "    'reg_lambda': tune.loguniform(1, 10),\n",
    "    'gamma': tune.uniform(0.5, 2)\n",
    "}\n",
    "lgbm_params = {\n",
    "    'reg_alpha': tune.loguniform(1, 10),\n",
    "    'reg_lambda': tune.loguniform(1, 10)\n",
    "}\n",
    "catboost_params = {\n",
    "    'reg_lambda': tune.loguniform(3, 10)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(max_features='sqrt'), boosting_params),\n",
    "    'XGBoost': (XGBClassifier(colsample_bynode=0.5, use_label_encoder=False,\n",
    "                              eval_metric='logloss', random_state=123), xgb_params),\n",
    "    'LightGBM': (LGBMClassifier(colsample_bynode=0.5), lgbm_params),\n",
    "    'CatBoost': (CatBoostClassifier(colsample_bylevel=0.5, n_estimators=100,\n",
    "                                    verbose=False, random_state=123), catboost_params)\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    'prostate': (X_1, y_1, models),\n",
    "    'leukemia': (X_2, y_2, models),\n",
    "    'weather': (X_3, y_3, models)\n",
    "}\n",
    "\n",
    "param_dict_nlp = {\n",
    "    'IMDB reviews': (X_4, y_4, models)\n",
    "}\n",
    "\n",
    "tfidf_kws = {'ngram_range': (1, 2), 'min_df': 3, 'max_features': 10000}\n",
    "\n",
    "all_results, all_runtimes, results_for_plotting, runtimes_for_plotting = run(param_dict=param_dict,\n",
    "                                                                             mode='TPE', scoring='accuracy')\n",
    "\n",
    "_, _, results_for_plotting_nlp, runtimes_for_plotting_nlp = run_nlp(param_dict=param_dict_nlp,\n",
    "                                                                    mode='TPE',\n",
    "                                                                    scoring='accuracy',\n",
    "                                                                    tfidf_kws=tfidf_kws\n",
    "                                                                    )\n",
    "\n",
    "name = 'high_dimensional_TPE'\n",
    "all_results = pd.concat([results_for_plotting, results_for_plotting_nlp])\n",
    "all_runtimes = pd.concat([runtimes_for_plotting, runtimes_for_plotting_nlp])\n",
    "all_results.to_excel(f'results_colab/results_{name}.xlsx', index=False)\n",
    "all_runtimes.to_excel(f'results_colab/runtimes_{name}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51928371",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(f'results_colab/results_{name}.xlsx')\n",
    "files.download(f'results_colab/runtimes_{name}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db633cc3",
   "metadata": {},
   "source": [
    " # Ordinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy import stats\n",
    "\n",
    "from wrappers.datasets_models_wrappers import DataModelsWrapper, DataModelsWrapperRandomSearch\n",
    "from data_processing.process_dataset import prepare_datasets_for_classification\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def run(param_dict, mode='randomized', tuner='hyperopt', scoring='accuracy'):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperRandomSearch(param_dict, scoring=scoring)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapper(param_dict, tuner=tuner, scoring=scoring)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'mushrooms.csv': ('class', 'all', None),\n",
    "    'adult.csv': ('profit', [], None),\n",
    "    'churn.csv': ('Churn', [], None),\n",
    "    'creditcard.csv': ('Class', [], None)\n",
    "}\n",
    "\n",
    "X_1, y_1, X_2, y_2, X_3, y_3, X_4, y_4 = prepare_datasets_for_classification(d, data_path='data/')\n",
    "\n",
    "boosting_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': stats.loguniform(0.01, 0.1)\n",
    "}\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': stats.loguniform(0.01, 0.1)\n",
    "}\n",
    "lgbm_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': stats.loguniform(0.01, 0.1)\n",
    "}\n",
    "catboost_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': stats.loguniform(0.01, 0.1)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(), boosting_params),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False,\n",
    "                             eval_metric='logloss', random_state=123), xgb_params),\n",
    "    'LightGBM': (LGBMClassifier(), lgbm_params),\n",
    "    'CatBoost': (CatBoostClassifier(verbose=False, random_state=123), catboost_params)\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    'mushrooms': (X_1, y_1, models),\n",
    "    'adult': (X_2, y_2, models),\n",
    "    'churn': (X_3, y_3, models),\n",
    "    'credit card': (X_4, y_4, models)\n",
    "}\n",
    "\n",
    "all_results, all_runtimes, results_for_plotting, runtimes_for_plotting = run(param_dict=param_dict,\n",
    "                                                                             mode='randomized', scoring='accuracy')\n",
    "\n",
    "name = 'ordinary'\n",
    "results_for_plotting.to_excel(f'../results/results_{name}.xlsx', index=False)\n",
    "runtimes_for_plotting.to_excel(f'../results/runtimes_{name}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(f'results_colab/results_{name}.xlsx')\n",
    "files.download(f'results_colab/runtimes_{name}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888637c",
   "metadata": {},
   "source": [
    "# High dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy import stats\n",
    "\n",
    "from wrappers.datasets_models_wrappers import DataModelsWrapper, DataModelsWrapperRandomSearch\n",
    "from wrappers.datasets_models_wrappers_nlp import DataModelsWrapperNLP, DataModelsWrapperNLPRandomSearch\n",
    "from data_processing.process_dataset import prepare_datasets_for_classification\n",
    "from data_processing.process_dataset_nlp import prepare_nlp_for_classification\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def run(param_dict, mode='randomized', tuner='hyperopt', scoring='accuracy'):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperRandomSearch(param_dict, scoring=scoring)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapper(param_dict, tuner=tuner, scoring=scoring)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting\n",
    "\n",
    "\n",
    "def run_nlp(param_dict, mode='randomized', tuner='hyperopt', scoring='accuracy',\n",
    "        tfidf_kws={'ngram_range': (1, 2), 'min_df': 3, 'max_features': 10000}):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperNLPRandomSearch(param_dict, scoring=scoring, tfidf_kws=tfidf_kws)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapperNLP(param_dict, tuner=tuner, scoring=scoring, tfidf_kws=tfidf_kws)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'prostate.csv': ('target', [], None),\n",
    "    'leukemia.csv': ('target', [], None),\n",
    "    'weather_dataset.csv': ('target', [], 500)\n",
    "}\n",
    "\n",
    "X_1, y_1, X_2, y_2, X_3, y_3 = prepare_datasets_for_classification(d, data_path='data/')\n",
    "\n",
    "X_4, y_4 = prepare_nlp_for_classification(\n",
    "    dataset_name='imdb_dataset.csv',\n",
    "    text_column='review_cleared',\n",
    "    y_col='sentiment',\n",
    "    nrows=3000,\n",
    "    data_path='data/'\n",
    ")\n",
    "\n",
    "boosting_params = {\n",
    "    'subsample': stats.uniform(0.6, 0.8)\n",
    "}\n",
    "xgb_params = {\n",
    "    'reg_alpha': stats.loguniform(1, 10),\n",
    "    'reg_lambda': stats.loguniform(1, 10),\n",
    "    'gamma': stats.uniform(0.5, 2)\n",
    "}\n",
    "lgbm_params = {\n",
    "    'reg_alpha': stats.loguniform(1, 10),\n",
    "    'reg_lambda': stats.loguniform(1, 10)\n",
    "}\n",
    "catboost_params = {\n",
    "    'reg_lambda': stats.loguniform(3, 10)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(max_features='sqrt'), boosting_params),\n",
    "    'XGBoost': (XGBClassifier(colsample_bynode=0.5, use_label_encoder=False,\n",
    "                              eval_metric='logloss', random_state=123), xgb_params),\n",
    "    'LightGBM': (LGBMClassifier(colsample_bynode=0.5), lgbm_params),\n",
    "    'CatBoost': (CatBoostClassifier(colsample_bylevel=0.5, n_estimators=100,\n",
    "                                    verbose=False, random_state=123), catboost_params)\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    'prostate': (X_1, y_1, models),\n",
    "    'leukemia': (X_2, y_2, models),\n",
    "    'weather': (X_3, y_3, models)\n",
    "}\n",
    "\n",
    "param_dict_nlp = {\n",
    "    'IMDB reviews': (X_4, y_4, models)\n",
    "}\n",
    "\n",
    "tfidf_kws = {'ngram_range': (1, 2), 'min_df': 3, 'max_features': 10000}\n",
    "\n",
    "all_results, all_runtimes, results_for_plotting, runtimes_for_plotting = run(param_dict=param_dict,\n",
    "                                                                             mode='randomized', scoring='accuracy')\n",
    "\n",
    "_, _, results_for_plotting_nlp, runtimes_for_plotting_nlp = run_nlp(param_dict=param_dict_nlp,\n",
    "                                                                    mode='randomized',\n",
    "                                                                    scoring='accuracy',\n",
    "                                                                    tfidf_kws=tfidf_kws\n",
    "                                                                    )\n",
    "\n",
    "name = 'high_dimensional'\n",
    "all_results = pd.concat([results_for_plotting, results_for_plotting_nlp])\n",
    "all_runtimes = pd.concat([runtimes_for_plotting, runtimes_for_plotting_nlp])\n",
    "all_results.to_excel(f'../results/results_{name}.xlsx', index=False)\n",
    "all_runtimes.to_excel(f'../results/runtimes_{name}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(f'results_colab/results_{name}.xlsx')\n",
    "files.download(f'results_colab/runtimes_{name}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898c2cc",
   "metadata": {},
   "source": [
    "# Runtimes dependent on data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d015f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from visualization.palettes import default_palette\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "def runtimes_features(models, n_features_list, n_samples=100):\n",
    "    cols = models.keys()\n",
    "    results = []\n",
    "    for n_features in n_features_list:\n",
    "        X, y = make_classification(n_samples=n_samples, n_features=n_features)\n",
    "        record = []\n",
    "        for model in models.copy().values():\n",
    "            t0 = time()\n",
    "            model.fit(X, y)\n",
    "            t = time()\n",
    "            record.append(t-t0)\n",
    "        results.append(record)\n",
    "    results = pd.DataFrame(results, columns=cols)\n",
    "    results['n_features'] = n_features_list\n",
    "    return results\n",
    "\n",
    "\n",
    "def runtimes_samples(models, n_samples_list, n_features=10):\n",
    "    cols = models.keys()\n",
    "    results = []\n",
    "    for n_samples in n_samples_list:\n",
    "        X, y = make_classification(n_samples=n_samples, n_features=n_features)\n",
    "        record = []\n",
    "        for model in models.copy().values():\n",
    "            t0 = time()\n",
    "            model.fit(X, y)\n",
    "            t = time()\n",
    "            record.append(t-t0)\n",
    "        results.append(record)\n",
    "    results = pd.DataFrame(results, columns=cols)\n",
    "    results['n_samples'] = n_samples_list\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_results(\n",
    "        results_features,\n",
    "        results_samples,\n",
    "        out_path='../plots/runtimes_features_samples.pdf',\n",
    "        figsize=(12, 8),\n",
    "        base=10,\n",
    "        save=False,\n",
    "        **kwargs\n",
    "):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    melted_features = results_features.melt(id_vars='n_features', value_name='runtime', var_name='model')\n",
    "    sns.lineplot(data=melted_features, x='n_features', y='runtime', hue='model', marker='o', ax=ax[0], **kwargs)\n",
    "    ax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x)}s'))\n",
    "    ax[0].set_xscale('log', base=base)\n",
    "    ax[0].set_xlabel(r'$n_{features}$')\n",
    "\n",
    "    melted_samples = results_samples.melt(id_vars='n_samples', value_name='runtime', var_name='model')\n",
    "    sns.lineplot(data=melted_samples, x='n_samples', y='runtime', hue='model', marker='o', ax=ax[1], **kwargs)\n",
    "    ax[1].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x)}s'))\n",
    "    ax[1].set_xscale('log', base=base)\n",
    "    ax[1].set_xlabel(r'$n_{samples}$')\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(out_path, bbox_inches='tight')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_estimators = 100\n",
    "    models = {\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=n_estimators),\n",
    "        'XGBoost': XGBClassifier(n_estimators=n_estimators, use_label_encoder=False, eval_metric='logloss', random_state=123),\n",
    "        'LightGBM': LGBMClassifier(n_estimators=n_estimators),\n",
    "        'CatBoost Ordered': CatBoostClassifier(boosting_type='Ordered', n_estimators=n_estimators,\n",
    "                                               verbose=False, random_state=123)\n",
    "        'CatBoost Plain': CatBoostClassifier(boosting_type='Plain', n_estimators=n_estimators,\n",
    "                                               verbose=False, random_state=123)\n",
    "    }\n",
    "\n",
    "    base = 5\n",
    "    n_features_list = base**np.arange(1, 7)\n",
    "    n_samples_list = base**np.arange(1, 9)\n",
    "\n",
    "    results_features = runtimes_features(models, n_features_list)\n",
    "    print('features done')\n",
    "    results_samples = runtimes_samples(models, n_samples_list)\n",
    "    print('samples done')\n",
    "    visualize_results(results_features, results_samples, base=base, palette='rainbow',\n",
    "                      save=True, out_path='../plots_colab/runtimes_features_samples.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7213ec",
   "metadata": {},
   "source": [
    "# LightGBM & XGBoost kwargs (legit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242375df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy import stats\n",
    "\n",
    "from wrappers.datasets_models_wrappers import DataModelsWrapper, DataModelsWrapperRandomSearch\n",
    "from data_processing.process_dataset import prepare_datasets_for_classification\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def run(param_dict, mode='randomized', tuner='hyperopt', scoring='accuracy'):\n",
    "    if mode == 'randomized':\n",
    "        model = DataModelsWrapperRandomSearch(param_dict, scoring=scoring)\n",
    "    elif mode == 'TPE':\n",
    "        model = DataModelsWrapper(param_dict, tuner=tuner, scoring=scoring)\n",
    "    model.fit()\n",
    "    all_results = model.all_datasets_results_\n",
    "    all_runtimes = model.all_datasets_runtimes_\n",
    "    results_for_plotting = model.results_for_plotting_\n",
    "    runtimes_for_plotting = model.runtimes_for_plotting_\n",
    "    return all_results, all_runtimes, results_for_plotting, runtimes_for_plotting\n",
    "\n",
    "\n",
    "X_1, y_1 = make_classification()\n",
    "\n",
    "xgb_params = {\n",
    "    'booster': ['dart', 'hist']\n",
    "}\n",
    "lgbm_params = {\n",
    "    'extra_trees': [True, False]\n",
    "}\n",
    "lgbm_params1 = {\n",
    "    'extra_trees': [False, True]\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'XGBoost': (XGBClassifier(booster='dart', rate_drop=.5, use_label_encoder=False,\n",
    "                              eval_metric='logloss', random_state=123), xgb_params),\n",
    "    'LightGBM': (LGBMClassifier(), lgbm_params),\n",
    "    'LightGBM2': (LGBMClassifier(), lgbm_params1)\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    'mushrooms': (X_1, y_1, models)\n",
    "}\n",
    "\n",
    "all_results, all_runtimes, results_for_plotting, runtimes_for_plotting = run(param_dict=param_dict,\n",
    "                                                                              mode='randomized', scoring='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boost",
   "language": "python",
   "name": "boost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
